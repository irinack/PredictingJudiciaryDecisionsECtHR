{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Judicial Decisions of the European Court of Human Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to train a classification model to classify cases as 'violation' or 'non-violation'. \n",
    "The cases were originally downloaded from HUDOC and structured based on the articles they fall under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang import en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy': make_scorer(sm.accuracy_score),\n",
    "           'precision': make_scorer(sm.precision_score),\n",
    "           'recall': make_scorer(sm.recall_score),\n",
    "           'f1': make_scorer(sm.f1_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read our dataset, we use os.walk to walk through a sub-tree of directories and files and load all of our training data and labels. We avoid the folder 'both' as the files inside are labelled both as violation and non-violation.\n",
    "Our data set will be loaded into dictionaries, the keys corresponding to articles and the values will be a list of cases (X - our training set) or labels (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(PATH):\n",
    "    X_dataset = {}\n",
    "    Y_dataset = {}\n",
    "    for path, dirs, files in os.walk(PATH):\n",
    "        for filename in files:\n",
    "            fullpath = os.path.join(path, filename)\n",
    "            if \"both\" not in fullpath:\n",
    "                with open(fullpath, 'r', encoding=\"utf8\") as file:\n",
    "                    X_dataset, Y_dataset = add_file_to_dataset(fullpath, X_dataset, Y_dataset, file.read())\n",
    "\n",
    "    return X_dataset, Y_dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_to_dataset(fullpath, x_dataset, y_dataset, file):\n",
    "    article = extract_article(fullpath)\n",
    "    file = preprocess(file)\n",
    "    if article not in x_dataset.keys() :\n",
    "        x_dataset[article] = []\n",
    "        y_dataset[article] = []\n",
    "    x_dataset[article] = x_dataset[article] + [file]\n",
    "    label = 0 if \"non-violation\" in fullpath else 1\n",
    "    y_dataset[article] = y_dataset[article] + [label]\n",
    "    return x_dataset, y_dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use regex to extract the number of the Article from the fullpath and insert the file into the list under that specific Article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article(path): \n",
    "    pattern = r\"(Article\\d+)\"\n",
    "    result = re.search(pattern, path)\n",
    "    article = result.group(1)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the research paper this work is based on, we will only use the PROCEDURE and THE FACTS paragraphs of the cases as our training set. Otherwise, the model may be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file): \n",
    "    file = extract_paragraphs(file)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(file): \n",
    "    file = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', file)\n",
    "    pat = r'(PROCEDURE\\s*\\n.+?)?((THE CIRCUMSTANCES OF THE CASE\\s*\\n.+?RELEVANT DOMESTIC LAW.+?)|(\\n(AS TO THE FACTS|THE FACTS)\\s*\\n.+?))(\\nIII\\.|THE LAW\\s*\\n|PROCEEDINGS BEFORE THE COMMISSION\\s*\\n|ALLEGED VIOLATION OF ARTICLE [0-9]+ OF THE CONVENTION \\s*\\n)'\n",
    "    result = re.search(pat, file, re.S |  re.IGNORECASE)\n",
    "    if result is None:\n",
    "        print(repr(file))\n",
    "    content = \"\"\n",
    "    if result.group(1) is not None:\n",
    "        content += result.group(1)\n",
    "    content += result.group(2)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"Datasets\\\\Human rights dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs, Y_train_docs = read_dataset(base_path + \"\\\\train\")\n",
    "#X_extra_test_docs, Y_extra_test = read_dataset(base_path + \"\\\\test_violations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Article10', 'Article11', 'Article12', 'Article13', 'Article14', 'Article18', 'Article2', 'Article3', 'Article4', 'Article5', 'Article6', 'Article7', 'Article8'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, similarly to Medvedeva, M., Vols, M. & Wieling, M. Artif Intell Law (2019), we want to remove the articles which contain too few cases. We include Article 11 \"as an estimate of how well the model performs when only very few cases are available\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_articles(train_set):\n",
    "    selected_training_set = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) <= 50:\n",
    "            selected_training_set.pop(key)\n",
    "            continue\n",
    "    return selected_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs = select_articles(X_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Article10', 'Article11', 'Article13', 'Article14', 'Article2', 'Article3', 'Article5', 'Article6', 'Article8'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all the articles according to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_docs[\"Article2\"] + X_train_docs[\"Article3\"] + X_train_docs[\"Article5\"] + X_train_docs[\"Article6\"] + X_train_docs[\"Article8\"] + X_train_docs[\"Article10\"] + X_train_docs[\"Article11\"] + X_train_docs[\"Article13\"] + X_train_docs[\"Article14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114+568+300+916+457+212+64+212+288=3131\n"
     ]
    }
   ],
   "source": [
    "print(str(len(X_train_docs[\"Article2\"])) + \"+\" + str(len(X_train_docs[\"Article3\"])) + \"+\" + str(len(X_train_docs[\"Article5\"])) + \"+\" + str(len(X_train_docs[\"Article6\"])) + \"+\" + str(len(X_train_docs[\"Article8\"])) + \"+\" + str(len(X_train_docs[\"Article10\"])) + \"+\" + str(len(X_train_docs[\"Article11\"])) + \"+\" + str(len(X_train_docs[\"Article13\"])) + \"+\" + str(len(X_train_docs[\"Article14\"])) + \"=\" + str(len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train_docs[\"Article2\"] + Y_train_docs[\"Article3\"] + Y_train_docs[\"Article5\"] + Y_train_docs[\"Article6\"] + Y_train_docs[\"Article8\"] + Y_train_docs[\"Article10\"] + Y_train_docs[\"Article11\"] + Y_train_docs[\"Article13\"] + Y_train_docs[\"Article14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3131"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the doc and lemmatize its tokens\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def my_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), lowercase=True, tokenizer=my_tokenizer, max_features=700000, binary=True, min_df=3)\n",
    "term_doc_matrix = vect.fit_transform(X_train).toarray()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features: 306631\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of features: \" + str(len(vect.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3131, 306631)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = SelectKBest(chi2, k=100000).fit_transform(term_doc_matrix, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3131, 100000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_instance = LogisticRegression(solver = 'lbfgs', C=0.5, max_iter=800)\n",
    "scores = cross_validate(classifier_instance, term_doc_matrix, Y_train, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.706\n",
      "Precision: 0.712\n",
      "Recall: 0.694\n",
      "F1: 0.696\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f\" % (scores[\"test_accuracy\"].mean()))\n",
    "print(\"Precision: %0.3f\" % (scores[\"test_precision\"].mean()))\n",
    "print(\"Recall: %0.3f\" % (scores[\"test_recall\"].mean()))\n",
    "print(\"F1: %0.3f\" % (scores[\"test_f1\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407.357695555687, 3.246254014968872)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"fit_time\"].mean(), scores[\"score_time\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.706 with LogisticRegression(solver = 'lbfgs', C=0.5, max_iter=800) and CountVectorizer(ngram_range=(1,2), lowercase=True, tokenizer=my_tokenizer, max_features=700000, binary=True, min_df=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with CountVectorizer and Classification with Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,3), lowercase=True, max_features=700000, binary=True, min_df=5)\n",
    "term_doc_matrix = vect.fit_transform(X_train).toarray()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features: 451342\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of features: \" + str(len(vect.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = SelectKBest(chi2, k=400000).fit_transform(term_doc_matrix, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3131, 400000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_instance = svm.LinearSVC(C=0.5, max_iter=1500)  \n",
    "scores = cross_validate(classifier_instance, term_doc_matrix, Y_train, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.721\n",
      "Precision: 0.727\n",
      "Recall: 0.707\n",
      "F1: 0.713\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f\" % (scores[\"test_accuracy\"].mean()))\n",
    "print(\"Precision: %0.3f\" % (scores[\"test_precision\"].mean()))\n",
    "print(\"Recall: %0.3f\" % (scores[\"test_recall\"].mean()))\n",
    "print(\"F1: %0.3f\" % (scores[\"test_f1\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155.54953446388245, 3.9933305978775024)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"fit_time\"].mean(), scores[\"score_time\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.721 with LinearSVC(C=0.5, max_iter=1500) and CountVectorizer(ngram_range=(1,3), lowercase=True, max_features=400000, binary=True, min_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with TfIdfVectorizer and Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,4), lowercase=True, tokenizer=my_tokenizer, max_features=600000, min_df=3)\n",
    "term_doc_matrix = vect.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features: 600000\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of features: \" + str(len(vect.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = SelectKBest(chi2, k=400000).fit_transform(term_doc_matrix, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3131, 400000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_instance = LogisticRegression()        \n",
    "scores = cross_validate(classifier_instance, term_doc_matrix, Y_train, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.710\n",
      "Precision: 0.731\n",
      "Recall: 0.690\n",
      "F1: 0.697\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f\" % (scores[\"test_accuracy\"].mean()))\n",
    "print(\"Precision: %0.3f\" % (scores[\"test_precision\"].mean()))\n",
    "print(\"Recall: %0.3f\" % (scores[\"test_recall\"].mean()))\n",
    "print(\"F1: %0.3f\" % (scores[\"test_f1\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094.1596326828003, 4.171499824523925)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"fit_time\"].mean(), scores[\"score_time\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.712 accuracy for LogisticRegression(solver = 'saga') with vect = TfidfVectorizer(ngram_range=(2,4), lowercase=True, tokenizer=my_tokenizer, max_features=400000, min_df=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with TfIdfVectorizer and Classification with Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5cf020d8c0a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_term_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_term_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(2,4), lowercase=True, max_features=600000, min_df=3)\n",
    "term_doc_matrix = vect.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features: 600000\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of features: \" + str(len(vect.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = SelectKBest(chi2, k=300000).fit_transform(term_doc_matrix, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3131, 300000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_instance = svm.LinearSVC(C=0.1, max_iter=1500)        \n",
    "scores = cross_validate(classifier_instance, X_train_new, Y_train, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744\n",
      "Precision: 0.766\n",
      "Recall: 0.722\n",
      "F1: 0.732\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.3f\" % (scores[\"test_accuracy\"].mean()))\n",
    "print(\"Precision: %0.3f\" % (scores[\"test_precision\"].mean()))\n",
    "print(\"Recall: %0.3f\" % (scores[\"test_recall\"].mean()))\n",
    "print(\"F1: %0.3f\" % (scores[\"test_f1\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155.90342404842377, 0.9028446197509765)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[\"fit_time\"].mean(), scores[\"score_time\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.744 accuracy with LinearSVC(C=0.1, max_iter=1500) and TfidfVectorizer(ngram_range=(2,4), lowercase=True, max_features=700000, min_df=3) with SelectKBest(chi2, k=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
