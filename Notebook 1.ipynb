{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Judicial Decisions of the European Court of Human Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to train a classification model to classify cases as 'violation' or 'non-violation'. \n",
    "The cases were originally downloaded from HUDOC and structured based on the articles they fall under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read our dataset, we use os.walk to walk through a sub-tree of directories and files and load all of our training data and labels. We avoid the folder 'both' as the files inside are labelled both as violation and non-violation.\n",
    "Our data set will be loaded into dictionaries, the keys corresponding to articles and the values will be a list of cases (X - our training set) or labels (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(PATH):\n",
    "    X_dataset = {}\n",
    "    Y_dataset = {}\n",
    "    for path, dirs, files in os.walk(PATH):\n",
    "        for filename in files:\n",
    "            fullpath = os.path.join(path, filename)\n",
    "            if \"both\" not in fullpath:\n",
    "                with open(fullpath, 'r', encoding=\"utf8\") as file:\n",
    "                    X_dataset, Y_dataset = add_file_to_dataset(fullpath, X_dataset, Y_dataset, file.read())\n",
    "\n",
    "    return X_dataset, Y_dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_to_dataset(fullpath, x_dataset, y_dataset, file):\n",
    "    article = extract_article(fullpath)\n",
    "    file = preprocess(file)\n",
    "    if article not in x_dataset.keys() :\n",
    "        x_dataset[article] = []\n",
    "        y_dataset[article] = []\n",
    "    x_dataset[article] = x_dataset[article] + [file]\n",
    "    label = 0 if \"non-violation\" in fullpath else 1\n",
    "    y_dataset[article] = y_dataset[article] + [label]\n",
    "    return x_dataset, y_dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use regex to extract the number of the Article from the fullpath and insert the file into the list under that specific Article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article(path): \n",
    "    pattern = r\"(Article\\d+)\"\n",
    "    result = re.search(pattern, path)\n",
    "    article = result.group(1)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the research paper this work is based on, we will only use the PROCEDURE and THE FACTS paragraphs of the cases as our training set. Otherwise, the model may be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file): \n",
    "    file = extract_paragraphs(file)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(file): \n",
    "    pat = r'((PROCEDURE|procedure\\n|THE FACTS|the facts\\n).+?)(III|THE LAW|the law\\n|PROCEEDINGS|ALLEGED VIOLATION OF ARTICLE)'\n",
    "    result = re.search(pat, file, re.S)\n",
    "    return result.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"Datasets\\\\Human rights dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs, Y_train = read_dataset(base_path + \"\\\\train\")\n",
    "#X_extra_test_docs, Y_extra_test = read_dataset(base_path + \"\\\\test_violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROCEDURE\\n1.\\xa0\\xa0The case originated in an application (no. 12773/03) against the Republic of Bulgaria '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_docs[\"Article2\"][0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROCEDURE\\n1.\\xa0\\xa0The case originated in an application (no. 42980/04) against the Republic of Bulgaria '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_docs[\"Article2\"][1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_docs[\"Article2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article2 = {'Article2': X_train_docs['Article2'], 'Label': Y_train['Article2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Article2  Label\n",
       "0  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "1  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "2  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "3  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "4  PROCEDURE\\n1.  The case originated in an appli...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article2_df = DataFrame(article2, columns= ['Article2', 'Label'])\n",
    "article2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>PROCEDURE\\n1.  The case originated in an appli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article2  Label\n",
       "47  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "65  PROCEDURE\\n1.  The case originated in an appli...      1\n",
       "38  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "26  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "85  PROCEDURE\\n1.  The case originated in an appli...      1\n",
       "..                                                ...    ...\n",
       "98  PROCEDURE\\n1.  The case originated in an appli...      1\n",
       "14  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "51  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "78  PROCEDURE\\n1.  The case originated in an appli...      1\n",
       "37  PROCEDURE\\n1.  The case originated in an appli...      0\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article2_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing our training set into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide our training set into a training and validation set (90%, 10%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, similarly to Medvedeva, M., Vols, M. & Wieling, M. Artif Intell Law (2019), we want to remove the articles which contain too few cases. We include Article 11 \"as an estimate of how well the model performs when only very few cases are available\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article 2 example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_article, X_test_article, Y_train_article, Y_test_article = train_test_split(X_train_docs[\"Article2\"], Y_train[\"Article2\"], test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROCEDURE\\n1.\\xa0\\xa0The case originated in seventeen applications (nos. 24093/14, 24104/14, 24106/14, 2410'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_article[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 12, 114)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_article), len(X_test_article), len(X_train_docs[\"Article2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_dataset(train_set, label_set):\n",
    "    divided_data_train = dict.fromkeys(train_set.keys(),[])\n",
    "    divided_data_test = dict.fromkeys(train_set.keys(),[])\n",
    "    divided_labels_train = dict.fromkeys(train_set.keys(),[])\n",
    "    divided_labels_test = dict.fromkeys(train_set.keys(),[])\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) < 50:\n",
    "            divided_data_train.pop(key)\n",
    "            divided_data_test.pop(key)\n",
    "            divided_labels_train.pop(key)\n",
    "            divided_labels_test.pop(key)\n",
    "            continue\n",
    "            \n",
    "        data_train_article, data_test_article, labels_train_article, labels_test_article = train_test_split(train_set[key], label_set[key], test_size=0.10, random_state=42)\n",
    "        \n",
    "        divided_data_train[key] = data_train_article\n",
    "        divided_data_test[key] = data_test_article\n",
    "        divided_labels_train[key] = labels_train_article\n",
    "        divided_labels_test[key] = labels_test_article\n",
    "    return divided_data_train, divided_data_test, divided_labels_train, divided_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key included:  Article10\n",
      "Key included:  Article11\n",
      "Key skipped over:  Article12\n",
      "Key included:  Article13\n",
      "Key included:  Article14\n",
      "Key skipped over:  Article18\n",
      "Key included:  Article2\n",
      "Key included:  Article3\n",
      "Key skipped over:  Article4\n",
      "Key included:  Article5\n",
      "Key included:  Article6\n",
      "Key included:  Article7\n",
      "Key included:  Article8\n"
     ]
    }
   ],
   "source": [
    "X_train_docs, X_test_docs, Y_train, Y_test = divide_dataset(X_train_docs, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Article10', 'Article11', 'Article13', 'Article14', 'Article2', 'Article3', 'Article5', 'Article6', 'Article7', 'Article8'])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_docs[\"Article2\"]), len(X_test_docs[\"Article2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  For article 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article2_term_doc = count_vect.fit_transform(X_train_docs[\"Article2\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a term-document matrix. The list of documents we provided to the CountVectorizer represent the rows, and the columns are the unique words (our vocabulary) which appear in the all the documents. The term-document matrix is a sparse matrix which counts the number of times each unique word has appeared in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 14694)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article2_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0  0 ...  0  0  0  0]\n",
      " [ 0  0  0  0 ...  0  0  0  0]\n",
      " [ 0 16  0  0 ...  0  0  0  0]\n",
      " [ 0  0  0  0 ...  0  0  0  0]\n",
      " ...\n",
      " [ 0  0  0  0 ...  0  0  0  0]\n",
      " [10  1  0  0 ...  0  0  0  0]\n",
      " [ 1  0  0  0 ...  0  0  0  0]\n",
      " [ 0  0  0  0 ...  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(article2_term_doc.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abetted',\n",
       " 'abetting',\n",
       " 'abide',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ablution',\n",
       " 'abnormal',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolishing',\n",
       " 'abolition',\n",
       " 'abort']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our vocabulary\n",
    "features = count_vect.get_feature_names()\n",
    "features[1030:1045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 14694)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_docs[\"Article2\"]), len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set, test_set):\n",
    "    dataset_vectorizer = copy.deepcopy(train_set)\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    test_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        vect = CountVectorizer()\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        test_term_doc = vect.transform(test_set[key]).toarray(test_set[key])\n",
    "        \n",
    "        dataset_vectorizer[key] = vect\n",
    "        train_term_doc_list[key] = article_term_doc\n",
    "        test_term_doc_list[key] = test_term_doc\n",
    "    return dataset_vectorizer, train_term_doc_list, test_term_doc_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CountVectorizer, X_train, X_test = tokenize_dataset(X_train_docs, X_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abetted',\n",
       " 'abetting',\n",
       " 'abide',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ablution',\n",
       " 'abnormal',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolishing',\n",
       " 'abolition',\n",
       " 'abort']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset_CountVectorizer[\"Article2\"].get_feature_names()\n",
    "features[1030:1045]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Article 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train[\"Article2\"], Y_train[\"Article2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the output\n",
    "y_test_pred = classifier.predict(X_test[\"Article2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance:\n",
      "Mean absolute error = 0.42\n",
      "Accuracy score = 0.5833333333333334\n",
      "F1 score = 0.4444444444444445\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted\n",
       "0        1          0\n",
       "1        0          1\n",
       "2        0          0\n",
       "3        1          1\n",
       "4        0          0\n",
       "5        0          1\n",
       "6        1          0\n",
       "7        1          0\n",
       "8        0          0\n",
       "9        0          0\n",
       "10       1          1\n",
       "11       0          0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute performance metrics\n",
    "print(\"Logistic regression performance:\")\n",
    "print(\"Mean absolute error =\", round(sm.mean_absolute_error(Y_test[\"Article2\"], y_test_pred), 2))\n",
    "# print(\"Mean squared error =\", round(sm.mean_squared_error(Y_test[\"Article2\"], y_test_pred), 2)) \n",
    "# print(\"Median absolute error =\", round(sm.median_absolute_error(Y_test[\"Article2\"], y_test_pred), 2)) \n",
    "# print(\"Explain variance score =\", round(sm.explained_variance_score(Y_test[\"Article2\"], y_test_pred), 2))\n",
    "# print(\"R2 score =\", round(sm.r2_score(Y_test[\"Article2\"], y_test_pred), 2))\n",
    "print(\"Accuracy score =\", sm.accuracy_score(Y_test[\"Article2\"], y_test_pred))\n",
    "print(\"F1 score =\", sm.f1_score(Y_test[\"Article2\"], y_test_pred))\n",
    "\n",
    "df = pd.DataFrame({'Actual': Y_test[\"Article2\"], 'Predicted': y_test_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LogReg(train_set, train_label_set, test_set, test_label_set):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "            classifier_instance = LogisticRegression()\n",
    "            classifier_instance.fit(train_set[key], train_label_set[key])\n",
    "            test_pred = classifier_instance.predict(test_set[key])\n",
    "            \n",
    "            print(\"Logistic regression performance for :\", key)\n",
    "            print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_label_set[key], test_pred), 2))\n",
    "            print(\"Accuracy score =\", round(sm.accuracy_score(test_label_set[key], test_pred), 2))\n",
    "            print(\"F1 score =\", round(sm.f1_score(test_label_set[key], test_pred), 2))\n",
    "            \n",
    "            accuracy = accuracy + round(sm.accuracy_score(test_label_set[key], test_pred), 2)\n",
    "    print(\"Average accuracy: \", accuracy / len(train_set.keys()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article10\n",
      "Mean absolute error = 0.36\n",
      "Accuracy score = 0.64\n",
      "F1 score = 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article11\n",
      "Mean absolute error = 0.43\n",
      "Accuracy score = 0.57\n",
      "F1 score = 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article13\n",
      "Mean absolute error = 0.14\n",
      "Accuracy score = 0.86\n",
      "F1 score = 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article14\n",
      "Mean absolute error = 0.28\n",
      "Accuracy score = 0.72\n",
      "F1 score = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article2\n",
      "Mean absolute error = 0.42\n",
      "Accuracy score = 0.58\n",
      "F1 score = 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article3\n",
      "Mean absolute error = 0.28\n",
      "Accuracy score = 0.72\n",
      "F1 score = 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article5\n",
      "Mean absolute error = 0.5\n",
      "Accuracy score = 0.5\n",
      "F1 score = 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article6\n",
      "Mean absolute error = 0.3\n",
      "Accuracy score = 0.7\n",
      "F1 score = 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article7\n",
      "Mean absolute error = 0.4\n",
      "Accuracy score = 0.6\n",
      "F1 score = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article8\n",
      "Mean absolute error = 0.37\n",
      "Accuracy score = 0.63\n",
      "F1 score = 0.56\n",
      "Average accuracy:  0.6519999999999999\n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LogReg(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 65.1% average accuracy for a simple classification model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LinearSVC(train_set, train_label_set, test_set, test_label_set):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "            classifier_instance = svm.LinearSVC()\n",
    "            classifier_instance.fit(train_set[key], train_label_set[key])\n",
    "            test_pred = classifier_instance.predict(test_set[key])\n",
    "            \n",
    "            print(\"Logistic regression performance for :\", key)\n",
    "            print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_label_set[key], test_pred), 2))\n",
    "            print(\"Accuracy score =\", round(sm.accuracy_score(test_label_set[key], test_pred), 2))\n",
    "            print(\"F1 score =\", round(sm.f1_score(test_label_set[key], test_pred), 2))\n",
    "            \n",
    "            accuracy = accuracy + round(sm.accuracy_score(test_label_set[key], test_pred), 2)\n",
    "    print(\"Average accuracy: \", accuracy / len(train_set.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article10\n",
      "Mean absolute error = 0.27\n",
      "Accuracy score = 0.73\n",
      "F1 score = 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article11\n",
      "Mean absolute error = 0.57\n",
      "Accuracy score = 0.43\n",
      "F1 score = 0.6\n",
      "Logistic regression performance for : Article13\n",
      "Mean absolute error = 0.14\n",
      "Accuracy score = 0.86\n",
      "F1 score = 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article14\n",
      "Mean absolute error = 0.28\n",
      "Accuracy score = 0.72\n",
      "F1 score = 0.75\n",
      "Logistic regression performance for : Article2\n",
      "Mean absolute error = 0.42\n",
      "Accuracy score = 0.58\n",
      "F1 score = 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article3\n",
      "Mean absolute error = 0.37\n",
      "Accuracy score = 0.63\n",
      "F1 score = 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article5\n",
      "Mean absolute error = 0.53\n",
      "Accuracy score = 0.47\n",
      "F1 score = 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article6\n",
      "Mean absolute error = 0.3\n",
      "Accuracy score = 0.7\n",
      "F1 score = 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article7\n",
      "Mean absolute error = 0.4\n",
      "Accuracy score = 0.6\n",
      "F1 score = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article8\n",
      "Mean absolute error = 0.37\n",
      "Accuracy score = 0.63\n",
      "F1 score = 0.59\n",
      "Average accuracy:  0.635\n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LinearSVC(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 63.5% average accuracy for a Support Vector Machine model (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
