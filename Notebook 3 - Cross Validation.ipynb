{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Judicial Decisions of the European Court of Human Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to train a classification model to classify cases as 'violation' or 'non-violation'. \n",
    "The cases were originally downloaded from HUDOC and structured based on the articles they fall under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read our dataset, we use os.walk to walk through a sub-tree of directories and files and load all of our training data and labels. We avoid the folder 'both' as the files inside are labelled both as violation and non-violation.\n",
    "Our data set will be loaded into dictionaries, the keys corresponding to articles and the values will be a list of cases (X - our training set) or labels (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(PATH):\n",
    "    X_dataset = {}\n",
    "    Y_dataset = {}\n",
    "    for path, dirs, files in os.walk(PATH):\n",
    "        for filename in files:\n",
    "            fullpath = os.path.join(path, filename)\n",
    "            if \"both\" not in fullpath:\n",
    "                with open(fullpath, 'r', encoding=\"utf8\") as file:\n",
    "                    X_dataset, Y_dataset = add_file_to_dataset(fullpath, X_dataset, Y_dataset, file.read())\n",
    "\n",
    "    return X_dataset, Y_dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_to_dataset(fullpath, x_dataset, y_dataset, file):\n",
    "    article = extract_article(fullpath)\n",
    "    file = preprocess(file)\n",
    "    if article not in x_dataset.keys() :\n",
    "        x_dataset[article] = []\n",
    "        y_dataset[article] = []\n",
    "    x_dataset[article] = x_dataset[article] + [file]\n",
    "    label = 0 if \"non-violation\" in fullpath else 1\n",
    "    y_dataset[article] = y_dataset[article] + [label]\n",
    "    return x_dataset, y_dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use regex to extract the number of the Article from the fullpath and insert the file into the list under that specific Article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article(path): \n",
    "    pattern = r\"(Article\\d+)\"\n",
    "    result = re.search(pattern, path)\n",
    "    article = result.group(1)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the research paper this work is based on, we will only use the PROCEDURE and THE FACTS paragraphs of the cases as our training set. Otherwise, the model may be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file): \n",
    "    file = extract_paragraphs(file)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(file): \n",
    "    pat = r'((PROCEDURE|procedure\\n|THE FACTS|the facts\\n).+?)(III|THE LAW|the law\\n|PROCEEDINGS|ALLEGED VIOLATION OF ARTICLE)'\n",
    "    result = re.search(pat, file, re.S)\n",
    "    return result.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"Datasets\\\\Human rights dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs, Y_train = read_dataset(base_path + \"\\\\train\")\n",
    "#X_extra_test_docs, Y_extra_test = read_dataset(base_path + \"\\\\test_violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, similarly to Medvedeva, M., Vols, M. & Wieling, M. Artif Intell Law (2019), we want to remove the articles which contain too few cases. We include Article 11 \"as an estimate of how well the model performs when only very few cases are available\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_articles(train_set):\n",
    "    selected_training_set = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) <= 50:\n",
    "            selected_training_set.pop(key)\n",
    "            continue\n",
    "    return selected_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs = select_articles(X_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Article10', 'Article11', 'Article13', 'Article14', 'Article2', 'Article3', 'Article5', 'Article6', 'Article8'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from spacy.lang import en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the doc and lemmatize its tokens\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def my_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set):\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        vect = CountVectorizer(ngram_range=(1,2), lowercase=True, tokenizer=my_tokenizer, max_features=700000, binary=True, min_df=3)\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        train_term_doc_list[key] = article_term_doc\n",
    "               \n",
    "        print(key)\n",
    "        print(\"The number of features: \" + str(len(vect.get_feature_names())))\n",
    "        print()\n",
    "        \n",
    "    return train_term_doc_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "The number of features: 34393\n",
      "\n",
      "Article11\n",
      "The number of features: 14412\n",
      "\n",
      "Article13\n",
      "The number of features: 39143\n",
      "\n",
      "Article14\n",
      "The number of features: 49597\n",
      "\n",
      "Article2\n",
      "The number of features: 28021\n",
      "\n",
      "Article3\n",
      "The number of features: 79063\n",
      "\n",
      "Article5\n",
      "The number of features: 44330\n",
      "\n",
      "Article6\n",
      "The number of features: 80337\n",
      "\n",
      "Article8\n",
      "The number of features: 68252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenize_dataset(X_train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy': make_scorer(sm.accuracy_score),\n",
    "           'precision': make_scorer(sm.precision_score),\n",
    "           'recall': make_scorer(sm.recall_score),\n",
    "           'f1': make_scorer(sm.f1_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_significance(params, predictions, X, Y):\n",
    "    newX = pd.DataFrame({\"Constant\": np.ones(len(X))}).join(pd.DataFrame(X))\n",
    "    MSE = (sum((Y - predictions) ** 2)) / (len(newX) - len(newX.columns))\n",
    "\n",
    "    # Note if you don't want to use a DataFrame replace the two lines above with\n",
    "    # newX = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    # MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))\n",
    "\n",
    "    var_b = MSE * (np.linalg.inv(np.dot(newX.T, newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params / sd_b\n",
    "\n",
    "    p_values =[2 * (1 - stats.t.cdf(np.abs(i), (len(newX) - 1))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b, 3)\n",
    "    ts_b = np.round(ts_b, 3)\n",
    "    p_values = np.round(p_values, 3)\n",
    "    params = np.round(params, 4)\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3[\"Coefficients\"], myDF3[\"Standard Errors\"], myDF3[\"t values\"], myDF3[\"Probabilites\"] = [params, sd_b, ts_b, p_values]\n",
    "    print(myDF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LogReg(train_set, train_label_set):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        print(key)\n",
    "        \n",
    "        classifier_instance = LogisticRegression(solver = 'lbfgs', max_iter=500)\n",
    "        #params = np.append(classifier_instance.intercept_,classifier_instance.coef_)\n",
    "        scores = cross_validate(classifier_instance, train_set[key], train_label_set[key], cv=10, scoring=scoring)\n",
    "        \n",
    "        #print(\"Statistical significance: \" + statistical_significance(params, ,train_set[key], train_label_set[key] ))\n",
    "        \n",
    "        print(\"Accuracy: %0.2f\" % (scores[\"test_accuracy\"].mean()))\n",
    "        print(\"Precision: %0.2f\" % (scores[\"test_precision\"].mean()))\n",
    "        print(\"Recall: %0.2f\" % (scores[\"test_recall\"].mean()))\n",
    "        print(\"F1: %0.2f\" % (scores[\"test_f1\"].mean()))   \n",
    "        print()\n",
    "            \n",
    "        accuracy = accuracy + scores[\"test_accuracy\"].mean()\n",
    "        precision = precision + scores[\"test_precision\"].mean()\n",
    "        recall = recall + scores[\"test_recall\"].mean()\n",
    "        f1 = f1 + scores[\"test_f1\"].mean()\n",
    "        \n",
    "    print(\"Average accuracy score:  %0.3f \" % (accuracy / len(train_set.keys())))\n",
    "    print(\"Average precision score:  %0.3f \" % (precision / len(train_set.keys())))\n",
    "    print(\"Average recall score:  %0.3f \" % (recall / len(train_set.keys())))\n",
    "    print(\"Average f1 score:  %0.3f \" % ( f1 / len(train_set.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "Accuracy: 0.54\n",
      "Precision: 0.51\n",
      "Recall: 0.58\n",
      "F1: 0.52\n",
      "\n",
      "Article11\n",
      "Accuracy: 0.77\n",
      "Precision: 0.82\n",
      "Recall: 0.84\n",
      "F1: 0.79\n",
      "\n",
      "Article13\n",
      "Accuracy: 0.76\n",
      "Precision: 0.76\n",
      "Recall: 0.77\n",
      "F1: 0.74\n",
      "\n",
      "Article14\n",
      "Accuracy: 0.70\n",
      "Precision: 0.73\n",
      "Recall: 0.74\n",
      "F1: 0.70\n",
      "\n",
      "Article2\n",
      "Accuracy: 0.62\n",
      "Precision: 0.68\n",
      "Recall: 0.59\n",
      "F1: 0.59\n",
      "\n",
      "Article3\n",
      "Accuracy: 0.73\n",
      "Precision: 0.75\n",
      "Recall: 0.76\n",
      "F1: 0.73\n",
      "\n",
      "Article5\n",
      "Accuracy: 0.64\n",
      "Precision: 0.66\n",
      "Recall: 0.67\n",
      "F1: 0.63\n",
      "\n",
      "Article6\n",
      "Accuracy: 0.68\n",
      "Precision: 0.68\n",
      "Recall: 0.68\n",
      "F1: 0.66\n",
      "\n",
      "Article8\n",
      "Accuracy: 0.67\n",
      "Precision: 0.67\n",
      "Recall: 0.69\n",
      "F1: 0.67\n",
      "\n",
      "Average accuracy score:  0.679 \n",
      "Average precision score:  0.696 \n",
      "Average recall score:  0.703 \n",
      "Average f1 score:  0.670 \n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LogReg(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 67.9% average accuracy for a Logistic Regression model(solver = 'lbfgs') with CountVectorizer(ngram_range=(1,2), lowercase=True, max_features=700000, tokenizer=my_tokenizer, binary=True, min_df=3) with 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set):\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) <= 50:\n",
    "            train_term_doc_list.pop(key)\n",
    "            continue\n",
    "        vect = CountVectorizer(ngram_range=(1,2), lowercase=True, max_features=700000, binary=True, min_df=3)\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        train_term_doc_list[key] = article_term_doc  \n",
    "        \n",
    "        print(key)\n",
    "        print(\"The number of features: \" + str(len(vect.get_feature_names())))\n",
    "        print()\n",
    "        \n",
    "    return train_term_doc_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "The number of features: 33811\n",
      "\n",
      "Article11\n",
      "The number of features: 13257\n",
      "\n",
      "Article13\n",
      "The number of features: 39376\n",
      "\n",
      "Article14\n",
      "The number of features: 50060\n",
      "\n",
      "Article2\n",
      "The number of features: 27229\n",
      "\n",
      "Article3\n",
      "The number of features: 84557\n",
      "\n",
      "Article5\n",
      "The number of features: 45591\n",
      "\n",
      "Article6\n",
      "The number of features: 85307\n",
      "\n",
      "Article8\n",
      "The number of features: 71839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenize_dataset(X_train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LinearSVC(train_set, train_label_set):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        print(key)\n",
    "        \n",
    "        classifier_instance = svm.LinearSVC(C=0.5)\n",
    "        scores = cross_validate(classifier_instance, train_set[key], train_label_set[key], cv=10, scoring=scoring)\n",
    "                \n",
    "        print(\"Accuracy: %0.2f\" % (scores[\"test_accuracy\"].mean()))\n",
    "        print(\"Precision: %0.2f\" % (scores[\"test_precision\"].mean()))\n",
    "        print(\"Recall: %0.2f\" % (scores[\"test_recall\"].mean()))\n",
    "        print(\"F1: %0.2f\" % (scores[\"test_f1\"].mean()))   \n",
    "        print()\n",
    "            \n",
    "        accuracy = accuracy + scores[\"test_accuracy\"].mean()\n",
    "        precision = precision + scores[\"test_precision\"].mean()\n",
    "        recall = recall + scores[\"test_recall\"].mean()\n",
    "        f1 = f1 + scores[\"test_f1\"].mean()\n",
    "        \n",
    "    print(\"Average accuracy score:  %0.3f \" % (accuracy / len(train_set.keys())))\n",
    "    print(\"Average precision score:  %0.3f \" % (precision / len(train_set.keys())))\n",
    "    print(\"Average recall score:  %0.3f \" % (recall / len(train_set.keys())))\n",
    "    print(\"Average f1 score:  %0.3f \" % ( f1 / len(train_set.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "Accuracy: 0.55\n",
      "Precision: 0.49\n",
      "Recall: 0.59\n",
      "F1: 0.52\n",
      "\n",
      "Article11\n",
      "Accuracy: 0.75\n",
      "Precision: 0.82\n",
      "Recall: 0.81\n",
      "F1: 0.77\n",
      "\n",
      "Article13\n",
      "Accuracy: 0.78\n",
      "Precision: 0.78\n",
      "Recall: 0.78\n",
      "F1: 0.76\n",
      "\n",
      "Article14\n",
      "Accuracy: 0.72\n",
      "Precision: 0.73\n",
      "Recall: 0.78\n",
      "F1: 0.74\n",
      "\n",
      "Article2\n",
      "Accuracy: 0.64\n",
      "Precision: 0.71\n",
      "Recall: 0.63\n",
      "F1: 0.62\n",
      "\n",
      "Article3\n",
      "Accuracy: 0.73\n",
      "Precision: 0.74\n",
      "Recall: 0.76\n",
      "F1: 0.73\n",
      "\n",
      "Article5\n",
      "Accuracy: 0.65\n",
      "Precision: 0.65\n",
      "Recall: 0.66\n",
      "F1: 0.63\n",
      "\n",
      "Article6\n",
      "Accuracy: 0.67\n",
      "Precision: 0.66\n",
      "Recall: 0.69\n",
      "F1: 0.65\n",
      "\n",
      "Article8\n",
      "Accuracy: 0.67\n",
      "Precision: 0.67\n",
      "Recall: 0.69\n",
      "F1: 0.67\n",
      "\n",
      "Average accuracy score:  0.684 \n",
      "Average precision score:  0.695 \n",
      "Average recall score:  0.709 \n",
      "Average f1 score:  0.676 \n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LinearSVC(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 68.4% average accuracy for a Support Vector Machine model (LinearSVC(C=0.5)) with CountVectorizer(ngram_range=(1,2), lowercase=True, max_features=700000, binary=True, min_df=3) with 10-fold CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set):\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) <= 50:\n",
    "            train_term_doc_list.pop(key)\n",
    "            continue\n",
    "        vect = TfidfVectorizer(ngram_range=(1,4), lowercase=True, max_features=950000)\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        train_term_doc_list[key] = article_term_doc\n",
    "        \n",
    "        print(key)\n",
    "        print(\"The number of features: \" + str(len(vect.get_feature_names())))\n",
    "        print()\n",
    "        \n",
    "    return train_term_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "The number of features: 950000\n",
      "\n",
      "Article11\n",
      "The number of features: 367293\n",
      "\n",
      "Article13\n",
      "The number of features: 950000\n",
      "\n",
      "Article14\n",
      "The number of features: 950000\n",
      "\n",
      "Article2\n",
      "The number of features: 893531\n",
      "\n",
      "Article3\n",
      "The number of features: 950000\n",
      "\n",
      "Article5\n",
      "The number of features: 950000\n",
      "\n",
      "Article6\n",
      "The number of features: 950000\n",
      "\n",
      "Article8\n",
      "The number of features: 950000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenize_dataset(X_train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LogReg(train_set, train_label_set):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        print(key)\n",
    "        \n",
    "        classifier_instance = LogisticRegression(solver = 'saga')\n",
    "        scores = cross_validate(classifier_instance, train_set[key], train_label_set[key], cv=10, scoring=scoring)\n",
    "                \n",
    "        print(\"Accuracy: %0.2f\" % (scores[\"test_accuracy\"].mean()))\n",
    "        print(\"Precision: %0.2f\" % (scores[\"test_precision\"].mean()))\n",
    "        print(\"Recall: %0.2f\" % (scores[\"test_recall\"].mean()))\n",
    "        print(\"F1: %0.2f\" % (scores[\"test_f1\"].mean()))      \n",
    "        print()\n",
    "            \n",
    "        accuracy = accuracy + scores[\"test_accuracy\"].mean()\n",
    "        precision = precision + scores[\"test_precision\"].mean()\n",
    "        recall = recall + scores[\"test_recall\"].mean()\n",
    "        f1 = f1 + scores[\"test_f1\"].mean()\n",
    "        \n",
    "    print(\"Average accuracy score:  %0.3f \" % (accuracy / len(train_set.keys())))\n",
    "    print(\"Average precision score:  %0.3f \" % (precision / len(train_set.keys())))\n",
    "    print(\"Average recall score:  %0.3f \" % (recall / len(train_set.keys())))\n",
    "    print(\"Average f1 score:  %0.3f \" % ( f1 / len(train_set.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "Accuracy: 0.56\n",
      "Precision: 0.56\n",
      "Recall: 0.53\n",
      "F1: 0.53\n",
      "\n",
      "Article11\n",
      "Accuracy: 0.75\n",
      "Precision: 0.81\n",
      "Recall: 0.81\n",
      "F1: 0.76\n",
      "\n",
      "Article13\n",
      "Accuracy: 0.76\n",
      "Precision: 0.77\n",
      "Recall: 0.75\n",
      "F1: 0.74\n",
      "\n",
      "Article14\n",
      "Accuracy: 0.67\n",
      "Precision: 0.72\n",
      "Recall: 0.58\n",
      "F1: 0.62\n",
      "\n",
      "Article2\n",
      "Accuracy: 0.72\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1: 0.71\n",
      "\n",
      "Article3\n",
      "Accuracy: 0.73\n",
      "Precision: 0.74\n",
      "Recall: 0.77\n",
      "F1: 0.73\n",
      "\n",
      "Article5\n",
      "Accuracy: 0.63\n",
      "Precision: 0.66\n",
      "Recall: 0.67\n",
      "F1: 0.64\n",
      "\n",
      "Article6\n",
      "Accuracy: 0.73\n",
      "Precision: 0.73\n",
      "Recall: 0.71\n",
      "F1: 0.71\n",
      "\n",
      "Article8\n",
      "Accuracy: 0.69\n",
      "Precision: 0.71\n",
      "Recall: 0.67\n",
      "F1: 0.68\n",
      "\n",
      "Average accuracy score:  0.694 \n",
      "Average precision score:  0.717 \n",
      "Average recall score:  0.693 \n",
      "Average f1 score:  0.680 \n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LogReg(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 69.4% average accuracy for a Logistic Regression (solver = 'saga') with TfidfVectorizer(ngram_range=(1,4), lowercase=True, max_features=950000) with 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set):\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) <= 50:\n",
    "            train_term_doc_list.pop(key)\n",
    "            continue\n",
    "        vect = TfidfVectorizer(ngram_range=(2,3), lowercase=True, max_features=900000, min_df=3)\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        train_term_doc_list[key] = article_term_doc\n",
    "        \n",
    "        print(key)\n",
    "        print(\"The number of features: \" + str(len(vect.get_feature_names())))\n",
    "        print()\n",
    "        \n",
    "    return train_term_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "The number of features: 50784\n",
      "\n",
      "Article11\n",
      "The number of features: 16815\n",
      "\n",
      "Article13\n",
      "The number of features: 64655\n",
      "\n",
      "Article14\n",
      "The number of features: 82692\n",
      "\n",
      "Article2\n",
      "The number of features: 40088\n",
      "\n",
      "Article3\n",
      "The number of features: 166397\n",
      "\n",
      "Article5\n",
      "The number of features: 83255\n",
      "\n",
      "Article6\n",
      "The number of features: 168068\n",
      "\n",
      "Article8\n",
      "The number of features: 132089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenize_dataset(X_train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LinearSVC(train_set, train_label_set):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        print(key)\n",
    "        \n",
    "        classifier_instance = svm.LinearSVC(C=0.1, max_iter=1500)\n",
    "        scores = cross_validate(classifier_instance, train_set[key], train_label_set[key], cv=10, scoring=scoring)\n",
    "                \n",
    "        print(\"Accuracy: %0.2f\" % (scores[\"test_accuracy\"].mean()))\n",
    "        print(\"Precision: %0.2f\" % (scores[\"test_precision\"].mean()))\n",
    "        print(\"Recall: %0.2f\" % (scores[\"test_recall\"].mean()))\n",
    "        print(\"F1: %0.2f\" % (scores[\"test_f1\"].mean()))      \n",
    "        print()\n",
    "            \n",
    "        accuracy = accuracy + scores[\"test_accuracy\"].mean()\n",
    "        precision = precision + scores[\"test_precision\"].mean()\n",
    "        recall = recall + scores[\"test_recall\"].mean()\n",
    "        f1 = f1 + scores[\"test_f1\"].mean()\n",
    "        \n",
    "    print(\"Average accuracy score:  %0.3f \" % (accuracy / len(train_set.keys())))\n",
    "    print(\"Average precision score:  %0.3f \" % (precision / len(train_set.keys())))\n",
    "    print(\"Average recall score:  %0.3f \" % (recall / len(train_set.keys())))\n",
    "    print(\"Average f1 score:  %0.3f \" % ( f1 / len(train_set.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article10\n",
      "Accuracy: 0.53\n",
      "Precision: 0.49\n",
      "Recall: 0.52\n",
      "F1: 0.50\n",
      "\n",
      "Article11\n",
      "Accuracy: 0.75\n",
      "Precision: 0.81\n",
      "Recall: 0.81\n",
      "F1: 0.76\n",
      "\n",
      "Article13\n",
      "Accuracy: 0.78\n",
      "Precision: 0.79\n",
      "Recall: 0.76\n",
      "F1: 0.76\n",
      "\n",
      "Article14\n",
      "Accuracy: 0.70\n",
      "Precision: 0.78\n",
      "Recall: 0.67\n",
      "F1: 0.68\n",
      "\n",
      "Article2\n",
      "Accuracy: 0.73\n",
      "Precision: 0.76\n",
      "Recall: 0.73\n",
      "F1: 0.72\n",
      "\n",
      "Article3\n",
      "Accuracy: 0.75\n",
      "Precision: 0.76\n",
      "Recall: 0.78\n",
      "F1: 0.75\n",
      "\n",
      "Article5\n",
      "Accuracy: 0.63\n",
      "Precision: 0.67\n",
      "Recall: 0.68\n",
      "F1: 0.63\n",
      "\n",
      "Article6\n",
      "Accuracy: 0.74\n",
      "Precision: 0.74\n",
      "Recall: 0.72\n",
      "F1: 0.72\n",
      "\n",
      "Article8\n",
      "Accuracy: 0.69\n",
      "Precision: 0.71\n",
      "Recall: 0.65\n",
      "F1: 0.67\n",
      "\n",
      "Average accuracy score:  0.700 \n",
      "Average precision score:  0.724 \n",
      "Average recall score:  0.702 \n",
      "Average f1 score:  0.687 \n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LinearSVC(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 70.0% average accuracy for a Support Vector Machine (LinearSVC(C=0.1, max_iter=1500)) with Tfidf vectorizer(ngram_range=(2,3), lowercase=True, max_features=800000, min_df=3) with 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
