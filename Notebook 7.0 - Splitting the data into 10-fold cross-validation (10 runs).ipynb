{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Notebook 7.0 - Splitting the data into 10-fold cross-validation (10 runs).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"p4Dc5AEJmBf6","colab_type":"text"},"source":["# Predicting Judicial Decisions of the European Court of Human Rights"]},{"cell_type":"code","metadata":{"id":"gmhlomnEmFzt","colab_type":"code","outputId":"729f8207-835c-4e1c-b7ce-0ad5f62bba1b","executionInfo":{"status":"ok","timestamp":1585748814287,"user_tz":-60,"elapsed":667,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-dbHND_mBf-","colab_type":"text"},"source":["In this notebook, we aim to train a classification model to classify cases as 'violation' or 'non-violation' using a Bert Sequence Classification model from the Transformer library. \n","The cases were originally downloaded from HUDOC and structured based on the articles they fall under."]},{"cell_type":"code","metadata":{"id":"TIBXb8ZCLNBV","colab_type":"code","outputId":"3c4baa83-7061-49c0-e8bb-6cee18dfa9ab","executionInfo":{"status":"ok","timestamp":1585748819040,"user_tz":-60,"elapsed":4856,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":423}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.7.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.31)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.31)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kOEPKbxLmBf_","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from transformers import *\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"81FX6H4smBgD","colab_type":"code","colab":{}},"source":["import numpy as np\n","import re\n","import os\n","import copy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rGK74I1mBgH","colab_type":"text"},"source":["To read our dataset, we use os.walk to walk through a sub-tree of directories and files and load all of our training data and labels. We avoid the folder 'both' as the files inside are labelled both as violation and non-violation.\n","Our data set will be loaded into dictionaries, the keys corresponding to articles and the values will be a list of cases (X - our training set) or labels (Y)."]},{"cell_type":"markdown","metadata":{"id":"1o4p4XPJSQLw","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"Y98FrLaLmBgJ","colab_type":"code","colab":{}},"source":["def read_dataset(PATH):\n","    X_dataset = {}\n","    Y_dataset = {}\n","    for path, dirs, files in os.walk(PATH):\n","        for filename in files:\n","            fullpath = os.path.join(path, filename)\n","            if \"both\" not in fullpath:\n","                with open(fullpath, 'r', encoding=\"utf8\") as file:\n","                    X_dataset, Y_dataset = add_file_to_dataset(fullpath, X_dataset, Y_dataset, file.read())\n","\n","    return X_dataset, Y_dataset       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-qXrKEYmBgM","colab_type":"code","colab":{}},"source":["def add_file_to_dataset(fullpath, x_dataset, y_dataset, file):\n","    article = extract_article(fullpath)\n","    file = preprocess(file)\n","    if article not in x_dataset.keys() :\n","        x_dataset[article] = []\n","        y_dataset[article] = []\n","    x_dataset[article] = x_dataset[article] + [file]\n","    label = 0 if \"non-violation\" in fullpath else 1\n","    y_dataset[article] = y_dataset[article] + [label]\n","    return x_dataset, y_dataset  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JT0ZlDkomBgQ","colab_type":"text"},"source":["We use regex to extract the number of the Article from the fullpath and insert the file into the list under that specific Article."]},{"cell_type":"code","metadata":{"id":"ipBBtAb-mBgR","colab_type":"code","colab":{}},"source":["def extract_article(path): \n","    pattern = r\"(Article\\d+)\"\n","    result = re.search(pattern, path)\n","    article = result.group(1)\n","    return article"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gZWfX5ZmBgV","colab_type":"text"},"source":["### Preprocessing "]},{"cell_type":"markdown","metadata":{"id":"Y8Ci_GoUmBgW","colab_type":"text"},"source":["Similar to the research paper this work is based on, we will only use the PROCEDURE and THE FACTS paragraphs of the cases as our training set. Otherwise, the model may be biased."]},{"cell_type":"code","metadata":{"id":"1JBouLcDmBgX","colab_type":"code","colab":{}},"source":["def preprocess(file): \n","    file = extract_paragraphs(file)\n","    return file"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B79LFOUNmBga","colab_type":"code","colab":{}},"source":["def extract_paragraphs(file): \n","  # Remove any non-ASCII characters\n","  file = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', file)\n","\n","  # # Remove any number at the beginning of a new line\n","  # pat = r'\\n[0-9].'\n","  # result = re.findall(pat, file, re.S | re.IGNORECASE)\n","  # for group in result:\n","  #     file = file.replace(group, \"\\n\")\n","\n","  # Extract THE FACTS paragraphs\n","  pat = r'((THE CIRCUMSTANCES OF THE CASE\\s*\\n.+?RELEVANT DOMESTIC LAW.+?)|(\\n(AS TO THE FACTS|THE FACTS)\\s*\\n.+?))(\\nIII\\.|THE LAW\\s*\\n|PROCEEDINGS BEFORE THE COMMISSION\\s*\\n|ALLEGED VIOLATION OF ARTICLE [0-9]+ OF THE CONVENTION \\s*\\n)'\n","  result = re.search(pat, file, re.S |  re.IGNORECASE)\n","\n","  content = \"\"\n","  content += result.group(1)\n","  return content"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJGzwDf8mBgd","colab_type":"text"},"source":["### Loading the data"]},{"cell_type":"code","metadata":{"id":"YXorjkQQmBgf","colab_type":"code","colab":{}},"source":["base_path = \"/content/drive/My Drive/Colab Notebooks/Datasets/Human rights dataset\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPxDT_yVmBgi","colab_type":"code","colab":{}},"source":["X_train_docs, Y_train_docs = read_dataset(base_path + \"/train\")\n","#X_extra_test_docs, Y_extra_test = read_dataset(base_path + \"\\\\test_violations\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FcPCAXlwmBgl","colab_type":"text"},"source":["Also, similarly to Medvedeva, M., Vols, M. & Wieling, M. Artif Intell Law (2019), we want to remove the articles which contain too few cases. We include Article 11 \"as an estimate of how well the model performs when only very few cases are available\"."]},{"cell_type":"code","metadata":{"id":"xsZllLGomBgl","colab_type":"code","colab":{}},"source":["def select_articles(train_set):\n","    selected_training_set = copy.deepcopy(train_set)\n","    \n","    for key in train_set.keys():\n","        if len(train_set[key]) <= 50:\n","            selected_training_set.pop(key)\n","            continue\n","    return selected_training_set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfsg4rhwmBgo","colab_type":"code","colab":{}},"source":["X_train_docs = select_articles(X_train_docs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UpdFUnf1LlD7","colab_type":"code","outputId":"6ae448a2-d22d-4a52-dba4-3618cfd656e6","executionInfo":{"status":"ok","timestamp":1585736357916,"user_tz":-60,"elapsed":18895,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(X_train_docs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kCCRgmMdmBgr","colab_type":"code","outputId":"917fc16b-5ac3-4ee8-9a7c-2e476cdeca20","executionInfo":{"status":"ok","timestamp":1585736357916,"user_tz":-60,"elapsed":18878,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(X_train_docs.keys())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dict_keys(['Article11', 'Article10', 'Article13', 'Article5', 'Article3', 'Article6', 'Article14', 'Article2', 'Article8'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5bX3o8qFmBgw","colab_type":"text"},"source":["### Combining all the articles according to class"]},{"cell_type":"code","metadata":{"id":"YUAG-FY5mBgx","colab_type":"code","colab":{}},"source":["X_train = X_train_docs[\"Article2\"] + X_train_docs[\"Article3\"] + X_train_docs[\"Article5\"] + X_train_docs[\"Article6\"] + X_train_docs[\"Article8\"] + X_train_docs[\"Article10\"] + X_train_docs[\"Article11\"] + X_train_docs[\"Article13\"] + X_train_docs[\"Article14\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Kwb162ZmBg0","colab_type":"code","colab":{}},"source":["Y_train = Y_train_docs[\"Article2\"] + Y_train_docs[\"Article3\"] + Y_train_docs[\"Article5\"] + Y_train_docs[\"Article6\"] + Y_train_docs[\"Article8\"] + Y_train_docs[\"Article10\"] + Y_train_docs[\"Article11\"] + Y_train_docs[\"Article13\"] + Y_train_docs[\"Article14\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBwL5AXamBg3","colab_type":"code","outputId":"9ad2a18b-4ab8-4cd8-903a-cdb8667af7a4","executionInfo":{"status":"ok","timestamp":1585736357918,"user_tz":-60,"elapsed":18845,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(Y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3131"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"MsLj_YtNmBg5","colab_type":"text"},"source":["### Activate logging"]},{"cell_type":"code","metadata":{"id":"H0a0_R1tmBg6","colab_type":"code","colab":{}},"source":["import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0ycarbOmBg8","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLRpsmZDmBg-","colab_type":"code","outputId":"7dfad9a0-53e4-4f4f-c320-365d80673c67","executionInfo":{"status":"ok","timestamp":1585748823843,"user_tz":-60,"elapsed":484,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["n_gpu = torch.cuda.device_count()\n","n_gpu"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"eUFPHrlwmBhB","colab_type":"code","outputId":"3afe4e03-47bd-4a9b-e77d-c334b3cca964","executionInfo":{"status":"ok","timestamp":1585748824197,"user_tz":-60,"elapsed":426,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.cuda.get_device_name(0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla P100-PCIE-16GB'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"_y-xrQMEmBhD","colab_type":"text"},"source":["## Training with Bert Model"]},{"cell_type":"markdown","metadata":{"id":"bm38Dxp_mBhE","colab_type":"text"},"source":["Credit to https://mccormickml.com/2019/07/22/BERT-fine-tuning/ for explaining and demonstrating how to train Bert"]},{"cell_type":"markdown","metadata":{"id":"JwFXGnVemBhF","colab_type":"text"},"source":["#### Tokenization"]},{"cell_type":"code","metadata":{"id":"YDj1qLf4mBhF","colab_type":"code","outputId":"0606affe-80dc-4882-d6fd-f973e4b25448","executionInfo":{"status":"ok","timestamp":1585736358092,"user_tz":-60,"elapsed":18907,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"BLsMrRsomBhH","colab_type":"text"},"source":["BERT has two constraints:\n","\n","* All sentences must be padded or truncated to a single, fixed length and must be formatted with special token ([CLS], [SEP])\n","* The maximum sentence length is 512 tokens."]},{"cell_type":"markdown","metadata":{"id":"DZ9NHkAXmBhI","colab_type":"text"},"source":["Because most of our sequence lengths are above 512 tokens, the tokenization step will only take the first 512 tokens. \n","In case there is a sequence that is less than 512 tokens, we might need to pad it until it reaches 512. "]},{"cell_type":"markdown","metadata":{"id":"XV-RgTZqmBhJ","colab_type":"text"},"source":["So, we need to find our maximum and minimum sentence lengths:"]},{"cell_type":"code","metadata":{"id":"8kfvPcwXmBhK","colab_type":"code","colab":{}},"source":["# max_len = 0\n","# avg = 0\n","# min_len = 999990000\n","\n","# for case in X_train:\n","#     tokens = tokenizer.tokenize(case)\n","#     max_len = max(len(tokens), max_len)\n","#     avg += len(tokens)\n","#     min_len = min(len(tokens), min_len)\n","\n","# print(\"Maximum tokens: \" + str(max_len)) ### 60252\n","# print(\"Average tokens: \" + str(avg/len(X_train))) ### 4266\n","# print(\"Minimum tokens: \" + str(min_len)) ### 118"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qr_s6jQYmBhM","colab_type":"code","colab":{}},"source":["# Tokenize all of the sentences, map the tokens to their word IDs and retrieve attention masks.\n","attention_masks = []\n","input_ids = []\n","\n","# `encode` will:\n","#   (1) Tokenize the sentence.\n","#   (2) Prepend the `[CLS]` token to the start.\n","#   (3) Append the `[SEP]` token to the end.\n","#   (4) Pad shorter sentences until they all have the maximum length.\n","#   (5) Map tokens to their IDs.\n","#   (6) Map which tokens are actual words versus which are padding.\n","encoded_sent = tokenizer.batch_encode_plus(\n","                    X_train,                   # list of sentences to encode.\n","                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                    pad_to_max_length = True,  # Add padding\n","                    max_length = 512\n","               )\n","    \n","# Retrieve attention mask and token IDs.\n","attention_masks = encoded_sent['attention_mask']\n","input_ids = encoded_sent['input_ids']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfSHEKIjvyFM","colab_type":"code","outputId":"db10672a-64ce-4fb9-806d-564d30faa339","executionInfo":{"status":"ok","timestamp":1585736816199,"user_tz":-60,"elapsed":476980,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(type(attention_masks))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'list'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QIoRTHbXKgTR","colab_type":"code","colab":{}},"source":["attention_masks_np = np.array(attention_masks)\n","input_ids_np = np.array(input_ids)\n","Y_train_np = np.array(Y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6oj9-c1UmBhV","colab_type":"text"},"source":["#### Training Set & Validation Set"]},{"cell_type":"code","metadata":{"id":"qU3xipNAv_QZ","colab_type":"code","outputId":"74ca38ef-283b-4304-817f-0244e1755ec6","executionInfo":{"status":"ok","timestamp":1585737464088,"user_tz":-60,"elapsed":6029,"user":{"displayName":"Irina Carabella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghc7fU0OvMy7bNgHyLPrhliac5JmVA2O29FjI3rZg=s64","userId":"14771962769165463145"}},"colab":{"base_uri":"https://localhost:8080/","height":857}},"source":["from sklearn.model_selection import StratifiedKFold\n","base_path = \"/content/drive/My Drive/Colab Notebooks/BertModel/\"\n","\n","runs = 0\n","kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n","for train_index, test_index in kfold.split(input_ids_np, Y_train_np):\n","    runs += 1\n","    version = \"\" + str(((runs / 10) + 1)) + '.' + str((runs % 10))\n","    #print(\"Version: \" + version)\n","\n","    # Retrieving the actual data based on index\n","    train_inputs, validation_inputs= input_ids_np[train_index], input_ids_np[test_index]\n","    train_labels, validation_labels = Y_train_np[train_index], Y_train_np[test_index]\n","\n","   \n","    # Converting to PyTorch Data Types\n","    train_inputs = torch.tensor(train_inputs)\n","    validation_inputs = torch.tensor(validation_inputs)\n","    #print(\"Shape: \" + str(validation_inputs.shape))\n","\n","    train_labels = torch.tensor(train_labels)\n","    validation_labels = torch.tensor(validation_labels)\n","    #print(\"Shape: \" + str(validation_labels.shape))\n","\n","\n","    # Saving our training data\n","    torch.save(train_inputs, base_path + 'train_inputs_' + str(version) + '.pt')\n","    torch.save(validation_inputs, base_path + 'validation_inputs_' + str(version) + '.pt')\n","\n","    torch.save(train_labels, base_path + 'train_labels_' + str(version) + '.pt')\n","    torch.save(validation_labels, base_path + 'validation_labels_' + str(version) + '.pt')\n","\n","runs = 0\n","for train_index, test_index in kfold.split(attention_masks_np, Y_train_np):\n","    runs += 1\n","    version = \"\" + str(((runs / 10) + 1)) + '.' + str((runs % 10))\n","    #print(\"Version: \" + version)\n","\n","\n","    train_masks, validation_masks= attention_masks_np[train_index], attention_masks_np[test_index]\n","\n","    train_masks = torch.tensor(train_masks)\n","    validation_masks = torch.tensor(validation_masks) \n","    #print(\"Shape: \" + str(validation_masks.shape))\n","\n","    torch.save(train_masks, base_path + 'train_masks_' + str(version) + '.pt')\n","    torch.save(validation_masks, base_path + 'validation_masks_' + str(version) + '.pt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Version: 1.1.1\n","Shape: torch.Size([314, 512])\n","Shape: torch.Size([314])\n","Version: 1.2.2\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.3.3\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.4.4\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.5.5\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.6.6\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.7.7\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.8.8\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.9.9\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 2.0.0\n","Shape: torch.Size([313, 512])\n","Shape: torch.Size([313])\n","Version: 1.1.1\n","Shape: torch.Size([314, 512])\n","Version: 1.2.2\n","Shape: torch.Size([313, 512])\n","Version: 1.3.3\n","Shape: torch.Size([313, 512])\n","Version: 1.4.4\n","Shape: torch.Size([313, 512])\n","Version: 1.5.5\n","Shape: torch.Size([313, 512])\n","Version: 1.6.6\n","Shape: torch.Size([313, 512])\n","Version: 1.7.7\n","Shape: torch.Size([313, 512])\n","Version: 1.8.8\n","Shape: torch.Size([313, 512])\n","Version: 1.9.9\n","Shape: torch.Size([313, 512])\n","Version: 2.0.0\n","Shape: torch.Size([313, 512])\n"],"name":"stdout"}]}]}