{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Judicial Decisions of the European Court of Human Rights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to train a classification model to classify cases as 'violation' or 'non-violation'. \n",
    "The cases were originally downloaded from HUDOC and structured based on the articles they fall under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read our dataset, we use os.walk to walk through a sub-tree of directories and files and load all of our training data and labels. We avoid the folder 'both' as the files inside are labelled both as violation and non-violation.\n",
    "Our data set will be loaded into dictionaries, the keys corresponding to articles and the values will be a list of cases (X - our training set) or labels (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(PATH):\n",
    "    X_dataset = {}\n",
    "    Y_dataset = {}\n",
    "    for path, dirs, files in os.walk(PATH):\n",
    "        for filename in files:\n",
    "            fullpath = os.path.join(path, filename)\n",
    "            if \"both\" not in fullpath:\n",
    "                with open(fullpath, 'r', encoding=\"utf8\") as file:\n",
    "                    X_dataset, Y_dataset = add_file_to_dataset(fullpath, X_dataset, Y_dataset, file.read())\n",
    "\n",
    "    return X_dataset, Y_dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_to_dataset(fullpath, x_dataset, y_dataset, file):\n",
    "    article = extract_article(fullpath)\n",
    "    file = preprocess(file)\n",
    "    if article not in x_dataset.keys() :\n",
    "        x_dataset[article] = []\n",
    "        y_dataset[article] = []\n",
    "    x_dataset[article] = x_dataset[article] + [file]\n",
    "    label = 0 if \"non-violation\" in fullpath else 1\n",
    "    y_dataset[article] = y_dataset[article] + [label]\n",
    "    return x_dataset, y_dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use regex to extract the number of the Article from the fullpath and insert the file into the list under that specific Article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article(path): \n",
    "    pattern = r\"(Article\\d+)\"\n",
    "    result = re.search(pattern, path)\n",
    "    article = result.group(1)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the research paper this work is based on, we will only use the PROCEDURE and THE FACTS paragraphs of the cases as our training set. Otherwise, the model may be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file): \n",
    "    file = extract_paragraphs(file)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(file): \n",
    "    pat = r'((PROCEDURE|procedure\\n|THE FACTS|the facts\\n).+?)(III|THE LAW|the law\\n|PROCEEDINGS|ALLEGED VIOLATION OF ARTICLE)'\n",
    "    result = re.search(pat, file, re.S)\n",
    "    return result.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"Datasets\\\\Human rights dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs, Y_train = read_dataset(base_path + \"\\\\train\")\n",
    "#X_extra_test_docs, Y_extra_test = read_dataset(base_path + \"\\\\test_violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing our training set into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide our training set into a training and validation set (90%, 10%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, similarly to Medvedeva, M., Vols, M. & Wieling, M. Artif Intell Law (2019), we want to remove the articles which contain too few cases. We include Article 11 \"as an estimate of how well the model performs when only very few cases are available\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_dataset(train_set, label_set):\n",
    "    divided_data_train = dict.fromkeys(train_set.keys(),[])\n",
    "    divided_data_test = dict.fromkeys(train_set.keys(),[])\n",
    "    divided_labels_train = dict.fromkeys(train_set.keys(),[])\n",
    "    divided_labels_test = dict.fromkeys(train_set.keys(),[])\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        if len(train_set[key]) <= 50:\n",
    "            divided_data_train.pop(key)\n",
    "            divided_data_test.pop(key)\n",
    "            divided_labels_train.pop(key)\n",
    "            divided_labels_test.pop(key)\n",
    "            continue\n",
    "            \n",
    "        data_train_article, data_test_article, labels_train_article, labels_test_article = train_test_split(train_set[key], label_set[key], test_size=0.10, random_state=42)\n",
    "        \n",
    "        divided_data_train[key] = data_train_article\n",
    "        divided_data_test[key] = data_test_article\n",
    "        divided_labels_train[key] = labels_train_article\n",
    "        divided_labels_test[key] = labels_test_article\n",
    "    return divided_data_train, divided_data_test, divided_labels_train, divided_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_docs, X_test_docs, Y_train, Y_test = divide_dataset(X_train_docs, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Article10', 'Article11', 'Article13', 'Article14', 'Article2', 'Article3', 'Article5', 'Article6', 'Article8'])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_docs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from spacy.lang import en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the doc and lemmatize its tokens\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def my_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set, test_set):\n",
    "    dataset_vectorizer = copy.deepcopy(train_set)\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    test_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        vect = CountVectorizer(ngram_range=(1,2), lowercase=True, max_features=700000, tokenizer=my_tokenizer, binary=True, min_df=3)\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        test_term_doc = vect.transform(test_set[key]).toarray(test_set[key])\n",
    "        \n",
    "        dataset_vectorizer[key] = vect\n",
    "        train_term_doc_list[key] = article_term_doc\n",
    "        test_term_doc_list[key] = test_term_doc\n",
    "    return dataset_vectorizer, train_term_doc_list, test_term_doc_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CountVectorizer, X_train, X_test = tokenize_dataset(X_train_docs, X_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- 22',\n",
       " '- 27',\n",
       " '- 30',\n",
       " '- 46',\n",
       " '- 5',\n",
       " '- 6',\n",
       " '- 66',\n",
       " '- 69',\n",
       " '- 7',\n",
       " '- 73',\n",
       " '- 8',\n",
       " '- 84',\n",
       " '- a',\n",
       " '- agent',\n",
       " '- and']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset_CountVectorizer[\"Article2\"].get_feature_names()\n",
    "features[1030:1045]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Ngrams and CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LogReg(train_set, train_label_set, test_set, test_label_set):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        \n",
    "        #classifier_instance = LogisticRegression(solver = 'saga', penalty='l1')\n",
    "        classifier_instance = LogisticRegression(solver = 'lbfgs')\n",
    "        classifier_instance.fit(train_set[key], train_label_set[key])\n",
    "        test_pred = classifier_instance.predict(test_set[key])\n",
    "\n",
    "        print(\"Logistic regression performance for :\", key)\n",
    "        print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_label_set[key], test_pred), 3))\n",
    "        print(\"Accuracy score =\", round(sm.accuracy_score(test_label_set[key], test_pred), 3))\n",
    "        print(\"F1 score =\", round(sm.f1_score(test_label_set[key], test_pred), 3))\n",
    "\n",
    "        accuracy = accuracy + round(sm.accuracy_score(test_label_set[key], test_pred), 3)\n",
    "            \n",
    "    print(\"Average accuracy: \", accuracy / len(train_set.keys()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article10\n",
      "Mean absolute error = 0.136\n",
      "Accuracy score = 0.864\n",
      "F1 score = 0.857\n",
      "Logistic regression performance for : Article11\n",
      "Mean absolute error = 0.286\n",
      "Accuracy score = 0.714\n",
      "F1 score = 0.75\n",
      "Logistic regression performance for : Article13\n",
      "Mean absolute error = 0.091\n",
      "Accuracy score = 0.909\n",
      "F1 score = 0.889\n",
      "Logistic regression performance for : Article14\n",
      "Mean absolute error = 0.207\n",
      "Accuracy score = 0.793\n",
      "F1 score = 0.786\n",
      "Logistic regression performance for : Article2\n",
      "Mean absolute error = 0.417\n",
      "Accuracy score = 0.583\n",
      "F1 score = 0.286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article3\n",
      "Mean absolute error = 0.228\n",
      "Accuracy score = 0.772\n",
      "F1 score = 0.755\n",
      "Logistic regression performance for : Article5\n",
      "Mean absolute error = 0.4\n",
      "Accuracy score = 0.6\n",
      "F1 score = 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article6\n",
      "Mean absolute error = 0.261\n",
      "Accuracy score = 0.739\n",
      "F1 score = 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article8\n",
      "Mean absolute error = 0.348\n",
      "Accuracy score = 0.652\n",
      "F1 score = 0.619\n",
      "Average accuracy:  0.7362222222222222\n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LogReg(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 73.6% average accuracy for a Logistic Regression model(solver = 'lbfgs') with CountVectorizer(ngram_range=(1,2), lowercase=True, max_features=700000, tokenizer=my_tokenizer, binary=True, min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LinearSVC(train_set, train_label_set, test_set, test_label_set):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        \n",
    "        classifier_instance = svm.LinearSVC(C=0.5)\n",
    "        classifier_instance.fit(train_set[key], train_label_set[key])\n",
    "        test_pred = classifier_instance.predict(test_set[key])\n",
    "\n",
    "        print(\"Logistic regression performance for :\", key)\n",
    "        print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_label_set[key], test_pred), 3))\n",
    "        print(\"Accuracy score =\", round(sm.accuracy_score(test_label_set[key], test_pred), 3))\n",
    "        print(\"F1 score =\", round(sm.f1_score(test_label_set[key], test_pred), 3))\n",
    "\n",
    "        accuracy = accuracy + round(sm.accuracy_score(test_label_set[key], test_pred), 3)\n",
    "            \n",
    "    print(\"Average accuracy: \", accuracy / len(train_set.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article10\n",
      "Mean absolute error = 0.182\n",
      "Accuracy score = 0.818\n",
      "F1 score = 0.8\n",
      "Logistic regression performance for : Article11\n",
      "Mean absolute error = 0.429\n",
      "Accuracy score = 0.571\n",
      "F1 score = 0.667\n",
      "Logistic regression performance for : Article13\n",
      "Mean absolute error = 0.091\n",
      "Accuracy score = 0.909\n",
      "F1 score = 0.889\n",
      "Logistic regression performance for : Article14\n",
      "Mean absolute error = 0.241\n",
      "Accuracy score = 0.759\n",
      "F1 score = 0.759\n",
      "Logistic regression performance for : Article2\n",
      "Mean absolute error = 0.417\n",
      "Accuracy score = 0.583\n",
      "F1 score = 0.286\n",
      "Logistic regression performance for : Article3\n",
      "Mean absolute error = 0.246\n",
      "Accuracy score = 0.754\n",
      "F1 score = 0.731\n",
      "Logistic regression performance for : Article5\n",
      "Mean absolute error = 0.4\n",
      "Accuracy score = 0.6\n",
      "F1 score = 0.667\n",
      "Logistic regression performance for : Article6\n",
      "Mean absolute error = 0.261\n",
      "Accuracy score = 0.739\n",
      "F1 score = 0.714\n",
      "Logistic regression performance for : Article8\n",
      "Mean absolute error = 0.391\n",
      "Accuracy score = 0.609\n",
      "F1 score = 0.591\n",
      "Average accuracy:  0.7046666666666667\n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LinearSVC(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 70.4% average accuracy for a Support Vector Machine model (LinearSVC(C=0.5)) with CountVectorizer(ngram_range=(1,2), lowercase=True, max_features=700000, tokenizer=my_tokenizer, binary=True, min_df=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Classification with TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(train_set, test_set):\n",
    "    dataset_vectorizer = copy.deepcopy(train_set)\n",
    "    train_term_doc_list = copy.deepcopy(train_set)\n",
    "    test_term_doc_list = copy.deepcopy(train_set)\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        vect = TfidfVectorizer(ngram_range=(2,4), lowercase=True, max_features=900000)\n",
    "        article_term_doc = vect.fit_transform(train_set[key]).toarray()    \n",
    "        test_term_doc = vect.transform(test_set[key]).toarray(test_set[key])\n",
    "        \n",
    "        dataset_vectorizer[key] = vect\n",
    "        train_term_doc_list[key] = article_term_doc\n",
    "        test_term_doc_list[key] = test_term_doc\n",
    "    return dataset_vectorizer, train_term_doc_list, test_term_doc_list   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_TfidfVectorizer, X_train, X_test = tokenize_dataset(X_train_docs, X_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08 it follows',\n",
       " '08 it follows from',\n",
       " '08 mikiyeva',\n",
       " '08 mikiyeva and',\n",
       " '08 mikiyeva and menchayeva',\n",
       " '08 of',\n",
       " '08 of 21',\n",
       " '08 of 21 july',\n",
       " '08 of 25',\n",
       " '08 of 25 february',\n",
       " '08 on',\n",
       " '08 on suspicion',\n",
       " '08 on suspicion of',\n",
       " '09 18',\n",
       " '09 18 december']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset_TfidfVectorizer[\"Article2\"].get_feature_names()\n",
    "features[1030:1045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LogReg(train_set, train_label_set, test_set, test_label_set):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "        classifier_instance = LogisticRegression(solver = 'lbfgs')\n",
    "        classifier_instance.fit(train_set[key], train_label_set[key])\n",
    "        test_pred = classifier_instance.predict(test_set[key])\n",
    "\n",
    "        print(\"Logistic regression performance for :\", key)\n",
    "        print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_label_set[key], test_pred), 3))\n",
    "        print(\"Accuracy score =\", round(sm.accuracy_score(test_label_set[key], test_pred), 3))\n",
    "        print(\"F1 score =\", round(sm.f1_score(test_label_set[key], test_pred), 3))\n",
    "\n",
    "        accuracy = accuracy + round(sm.accuracy_score(test_label_set[key], test_pred), 3)\n",
    "            \n",
    "    print(\"Average accuracy: \", accuracy / len(train_set.keys()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article10\n",
      "Mean absolute error = 0.182\n",
      "Accuracy score = 0.818\n",
      "F1 score = 0.75\n",
      "Logistic regression performance for : Article11\n",
      "Mean absolute error = 0.143\n",
      "Accuracy score = 0.857\n",
      "F1 score = 0.889\n",
      "Logistic regression performance for : Article13\n",
      "Mean absolute error = 0.136\n",
      "Accuracy score = 0.864\n",
      "F1 score = 0.842\n",
      "Logistic regression performance for : Article14\n",
      "Mean absolute error = 0.207\n",
      "Accuracy score = 0.793\n",
      "F1 score = 0.75\n",
      "Logistic regression performance for : Article2\n",
      "Mean absolute error = 0.25\n",
      "Accuracy score = 0.75\n",
      "F1 score = 0.667\n",
      "Logistic regression performance for : Article3\n",
      "Mean absolute error = 0.263\n",
      "Accuracy score = 0.737\n",
      "F1 score = 0.746\n",
      "Logistic regression performance for : Article5\n",
      "Mean absolute error = 0.367\n",
      "Accuracy score = 0.633\n",
      "F1 score = 0.703\n",
      "Logistic regression performance for : Article6\n",
      "Mean absolute error = 0.239\n",
      "Accuracy score = 0.761\n",
      "F1 score = 0.725\n",
      "Logistic regression performance for : Article8\n",
      "Mean absolute error = 0.348\n",
      "Accuracy score = 0.652\n",
      "F1 score = 0.579\n",
      "Average accuracy:  0.7627777777777778\n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LogReg(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 76.27% average accuracy for a Logistic Regression (solver = 'lbfgs') with TfidfVectorizer(ngram_range=(2,4), lowercase=True, max_features=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_classify_LinearSVC(train_set, train_label_set, test_set, test_label_set):\n",
    "    accuracy = 0\n",
    "    \n",
    "    for key in train_set.keys():\n",
    "            classifier_instance = svm.LinearSVC(C=0.1, max_iter=1500)\n",
    "            classifier_instance.fit(train_set[key], train_label_set[key])\n",
    "            test_pred = classifier_instance.predict(test_set[key])\n",
    "            \n",
    "            print(\"Logistic regression performance for :\", key)\n",
    "            print(\"Mean absolute error =\", round(sm.mean_absolute_error(test_label_set[key], test_pred), 3))\n",
    "            print(\"Accuracy score =\", round(sm.accuracy_score(test_label_set[key], test_pred), 3))\n",
    "            print(\"F1 score =\", round(sm.f1_score(test_label_set[key], test_pred), 3))\n",
    "            \n",
    "            accuracy = accuracy + round(sm.accuracy_score(test_label_set[key], test_pred), 3)\n",
    "            \n",
    "    print(\"Average accuracy: \", accuracy / len(train_set.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression performance for : Article10\n",
      "Mean absolute error = 0.182\n",
      "Accuracy score = 0.818\n",
      "F1 score = 0.75\n",
      "Logistic regression performance for : Article11\n",
      "Mean absolute error = 0.143\n",
      "Accuracy score = 0.857\n",
      "F1 score = 0.889\n",
      "Logistic regression performance for : Article13\n",
      "Mean absolute error = 0.136\n",
      "Accuracy score = 0.864\n",
      "F1 score = 0.842\n",
      "Logistic regression performance for : Article14\n",
      "Mean absolute error = 0.207\n",
      "Accuracy score = 0.793\n",
      "F1 score = 0.75\n",
      "Logistic regression performance for : Article2\n",
      "Mean absolute error = 0.25\n",
      "Accuracy score = 0.75\n",
      "F1 score = 0.667\n",
      "Logistic regression performance for : Article3\n",
      "Mean absolute error = 0.263\n",
      "Accuracy score = 0.737\n",
      "F1 score = 0.746\n",
      "Logistic regression performance for : Article5\n",
      "Mean absolute error = 0.367\n",
      "Accuracy score = 0.633\n",
      "F1 score = 0.703\n",
      "Logistic regression performance for : Article6\n",
      "Mean absolute error = 0.239\n",
      "Accuracy score = 0.761\n",
      "F1 score = 0.725\n",
      "Logistic regression performance for : Article8\n",
      "Mean absolute error = 0.326\n",
      "Accuracy score = 0.674\n",
      "F1 score = 0.615\n",
      "Average accuracy:  0.7652222222222222\n"
     ]
    }
   ],
   "source": [
    "dataset_LogReg = dataset_classify_LinearSVC(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 76.52% average accuracy for a Support Vector Machine (LinearSVC(C=0.1, max_iter=1500)) with Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
